{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Learning tensorflow deeper than model.fit()\n\nExperienced once the power of custom models in pytorch I wanted to impliment the same CapsNet-ish model in tensorflow. Fair enough, I did not suceed. At the time of writting the last trace is as follows:\n\n"},{"metadata":{},"cell_type":"markdown","source":"```NotImplementedError: Cannot convert a symbolic Tensor (encoder_17/primary_caps_17/conv2d_33/BiasAdd:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported```"},{"metadata":{},"cell_type":"markdown","source":"For now I simply have no time left to dig into the wood. Still I find it much more interesting than define vanila keras model than fit-predict. (I mean I can just Ctrl+C, Ctrl+V some of my github repo, that's not learning though). The model I tried to build is the same as I have build in model.py file, I hope at least my attempts will make your day :D"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\nimport tensorflow.keras.layers as layers\nimport tensorflow.keras.metrics as metrics\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","execution_count":67,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def load_mnist():\n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n    x_train = x_train/255\n    x_test = x_test/255\n    return x_train, y_train, x_test, y_test","execution_count":68,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, y_train, x_test, y_test = load_mnist()","execution_count":69,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(x_train[44])\nplt.title(y_train[44])","execution_count":70,"outputs":[{"output_type":"execute_result","execution_count":70,"data":{"text/plain":"Text(0.5, 1.0, '3')"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(np.rot90(x_train[44], 3))\nplt.title(y_train[44])","execution_count":71,"outputs":[{"output_type":"execute_result","execution_count":71,"data":{"text/plain":"Text(0.5, 1.0, '3')"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAOBUlEQVR4nO3df7BU9XnH8c9HREhRG0GxDDLRWGpL2oLlhoSQiabWVGgykExiZWqGTmyxbUyTTqbWSf/Aaf8xncaMbYwpRibkl8ZpYkWHpjr0B2NjqRdDAEXFOhh+3EIzZIqx9gry9I976FzgnrPX3bN79vq8XzM7u3uePXueOfC55+yec/briBCAN74zmm4AQG8QdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1jsv1120O2j9h+zvbvNN0TOmNOqsFYbL9N0vMRMWz75yX9s6TfiIitzXaGdrFlx5gi4qmIGD7xtLhd2mBL6BBhRynbX7T9P5KekTQkaWPDLaED7Majku1JkhZLulLSZyPiaLMdoV1s2VEpIl6LiMckXSTp95vuB+0j7BivM8Vn9gmNsOM0tmfavs722bYn2f51SSsl/WPTvaF9fGbHaWxfIOlvJc3XyAbhRUl/FRF3N9oYOkLYgSTYjQeSIOxAEoQdSIKwA0mc2cuFneUpMVXTerlIIJX/1ct6NYY9Vq2jsNu+RtIdkiZJ+nJE3Fb1+qmapnf4qk4WCaDClthUWmt7N744Z/pOSUslzZO00va8dt8PQHd18pl9kUaud34hIl6VdJ+k5fW0BaBunYR9tqS9o57vK6adxPZq24O2B49q+NQygB7pJOxjfQlw2ul4EbE2IgYiYmCypnSwOACd6CTs+yTNGfX8IkkHOmsHQLd0EvYnJM21fYntsyRdJ2lDPW0BqFvbh94i4pjtmyT9g0YOva2LiKdq6wxArTo6zh4RG8XvkgETAqfLAkkQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5LoaMhm23skvSTpNUnHImKgjqYA1K+jsBfeGxE/quF9AHQRu/FAEp2GPSQ9Ynur7dVjvcD2atuDtgeParjDxQFoV6e78Usi4oDtmZIetf1MRGwe/YKIWCtprSSd6+nR4fIAtKmjLXtEHCjuD0l6QNKiOpoCUL+2w257mu1zTjyW9D5JO+tqDEC9OtmNv1DSA7ZPvM83I+K7tXQF1GDh94+X1h5+8W2V885asavudhrXdtgj4gVJ82vsBUAXcegNSIKwA0kQdiAJwg4kQdiBJOq4EAZoxP5b3lVZf3jmX5fW7ntscd3t9D227EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBMfZMWG9/LOvVtaPq/wS10seOFZ3O32PLTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJMFxdkxYKxf+e2X9DLZlJ2FtAEkQdiAJwg4kQdiBJAg7kARhB5Ig7EASHGfHG1bV9ewZtdyy215n+5DtnaOmTbf9qO3dxf153W0TQKfGsxv/FUnXnDLtFkmbImKupE3FcwB9rGXYI2KzpMOnTF4uaX3xeL2kFTX3BaBm7X5Bd2FEDElScT+z7IW2V9setD14VMNtLg5Ap7r+bXxErI2IgYgYmKwp3V4cgBLthv2g7VmSVNwfqq8lAN3Qbtg3SFpVPF4l6cF62gHQLS2Ps9u+V9KVks63vU/SGkm3Sbrf9g2SfijpI91s8oRjv7qwtLb/iuqPCG9Z87262+kbryxfVFo78JvVv61+5u6fqqxP5PXG9ewnaxn2iFhZUrqq5l4AdBF/+oAkCDuQBGEHkiDsQBKEHUhiQl3i+mdfvru0dvNzH+5hJ/1l7wfKL+V85orydSZJZ1xR/fd+3ptuqqy/9ebHK+tN4hLXk7FlB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkJtRx9lWP31BaO374rMp55+qFutvpG1VDFz/88ozKeSe5+lj007/1hcr6ZW/6g8r63E9sqax3E5e4noy1ASRB2IEkCDuQBGEHkiDsQBKEHUiCsANJTKjj7DM3lP9c9L/cfmflvAvnXF9Zn7ViV1s99YLf/kuV9d+bcVdpbdmdN1fOO+PpY5X1pV+qXq/PfuiLlfV5r5RfD9/ta+G5nv1kbNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IIkJdZz9nG/9W2nt566+sXLe55b+TWX9vd+tHnX6p3+3fOjjY3v3Vc7bqSOXTqusz55UPexylakPlV8LL0kf+v6Kyvr8h/ZW1quuh598/aTKea+6vvz3CyRp9pRHKutV1/JP3X2wct7qsw8mppZbdtvrbB+yvXPUtFtt77e9rbgt626bADo1nt34r0i6Zozpn4+IBcVtY71tAahby7BHxGZJh3vQC4Au6uQLuptsby92888re5Ht1bYHbQ8e1XAHiwPQiXbDfpekSyUtkDQk6XNlL4yItRExEBEDk1V+IQuA7mor7BFxMCJei4jjku6WtKjetgDUra2w25416ukHJe0sey2A/tDyOLvteyVdKel82/skrZF0pe0FkkLSHknVB7l7YN6tQ5X198y+trK++Zfvr6yv2XB5aW3Dfe+unHf2Z79XWW/lx5dV/00+riittbpevZVj+/ZX1n/wgTmV9cv++F2ltWc/XH2t/Mavfamy3up34dccKv836/a5Ef2oZdgjYuUYk+/pQi8AuojTZYEkCDuQBGEHkiDsQBKEHUhiQl3iWqXVIaJzl1bPv/Dvqn9qeuvbv15a+/M/3FY572XzP1ZZv+ChqZX16Yv/s7J+hlxam3rwlcp5O9Vqvc/9ZHl98fbyn5mWpOH3/3dlffklOyrrW2+cX1GtnveNiC07kARhB5Ig7EAShB1IgrADSRB2IAnCDiThiPLLI+t2rqfHO3xVz5ZXp913vLO09ke/9veV865+8/OV9VaXarYaerhq/vfPXlg5L95YtsQmHYnDY554wZYdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5LgOHsPHP7Y4sr61Z/4147e/5EvLCmtzbjn8Y7eGxMLx9kBEHYgC8IOJEHYgSQIO5AEYQeSIOxAEi2Ps9ueI+mrkn5G0nFJayPiDtvTJX1L0sUaGbb52oj4cdV7ZT3ODvRKp8fZj0n6dET8gqR3Svq47XmSbpG0KSLmStpUPAfQp1qGPSKGIuLJ4vFLknZJmi1puaT1xcvWS1rRrSYBdO51fWa3fbGkyyVtkXRhRAxJI38QJM2suzkA9Rl32G2fLenbkj4VEUdex3yrbQ/aHjyq4XZ6BFCDcYXd9mSNBP0bEfGdYvJB27OK+ixJh8aaNyLWRsRARAxM1pQ6egbQhpZht21J90jaFRG3jyptkLSqeLxK0oP1twegLuMZsnmJpI9K2mH7xNjEn5F0m6T7bd8g6YeSPtKdFgHUoWXYI+IxqXQAcA6aAxMEZ9ABSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkmgZdttzbP+T7V22n7L9yWL6rbb3295W3JZ1v10A7Wo5PrukY5I+HRFP2j5H0lbbjxa1z0fEX3avPQB1aRn2iBiSNFQ8fsn2Lkmzu90YgHq9rs/sti+WdLmkLcWkm2xvt73O9nkl86y2PWh78KiGO2oWQPvGHXbbZ0v6tqRPRcQRSXdJulTSAo1s+T831nwRsTYiBiJiYLKm1NAygHaMK+y2J2sk6N+IiO9IUkQcjIjXIuK4pLslLepemwA6NZ5v4y3pHkm7IuL2UdNnjXrZByXtrL89AHUZz7fxSyR9VNIO29uKaZ+RtNL2AkkhaY+kG7vSIYBajOfb+MckeYzSxvrbAdAtnEEHJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IwhHRu4XZ/yXpxVGTzpf0o5418Pr0a2/92pdEb+2qs7e3RMQFYxV6GvbTFm4PRsRAYw1U6Nfe+rUvid7a1ave2I0HkiDsQBJNh31tw8uv0q+99WtfEr21qye9NfqZHUDvNL1lB9AjhB1IopGw277G9rO2n7d9SxM9lLG9x/aOYhjqwYZ7WWf7kO2do6ZNt/2o7d3F/Zhj7DXUW18M410xzHij667p4c97/pnd9iRJz0m6WtI+SU9IWhkRT/e0kRK290gaiIjGT8Cw/R5JP5H01Yj4xWLaX0g6HBG3FX8oz4uIP+mT3m6V9JOmh/EuRiuaNXqYcUkrJP22Glx3FX1dqx6stya27IskPR8RL0TEq5Luk7S8gT76XkRslnT4lMnLJa0vHq/XyH+WnivprS9ExFBEPFk8fknSiWHGG113FX31RBNhny1p76jn+9Rf472HpEdsb7W9uulmxnBhRAxJI/95JM1suJ9TtRzGu5dOGWa8b9ZdO8Ofd6qJsI81lFQ/Hf9bEhG/ImmppI8Xu6sYn3EN490rYwwz3hfaHf68U02EfZ+kOaOeXyTpQAN9jCkiDhT3hyQ9oP4bivrgiRF0i/tDDffz//ppGO+xhhlXH6y7Joc/byLsT0iaa/sS22dJuk7Shgb6OI3tacUXJ7I9TdL71H9DUW+QtKp4vErSgw32cpJ+Gca7bJhxNbzuGh/+PCJ6fpO0TCPfyP+HpD9tooeSvt4q6QfF7amme5N0r0Z2645qZI/oBkkzJG2StLu4n95HvX1N0g5J2zUSrFkN9fZujXw03C5pW3Fb1vS6q+irJ+uN02WBJDiDDkiCsANJEHYgCcIOJEHYgSQIO5AEYQeS+D/7vjrtkiVlOAAAAABJRU5ErkJggg==\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PrimaryCaps(Model):\n    \n    def __init__(self, num_capsules=2,\n                 # in_channels=32, \n                 out_channels=16, \n                 kernel_size=9, \n                 num_routes=16*6*6):\n        \n        super().__init__()\n        self.num_routes = num_routes\n        self.capsule = layers.Conv2D(out_channels, \n                                     (kernel_size, kernel_size),\n                                     strides=(2, 2))\n\n    def call(self, x):\n        print(x.shape)\n        caps = [self.capsule(x) for _ in range(self.num_routes)]\n        caps = np.stack(caps, axis=1)\n        x = caps.reshape(\n            caps.shape[0], self.num_routes, -1)\n        return self.squash(x)\n        \n    \n    def squash(self, x):\n        v = (np.norm(x)**2/(\n            1+np.norm(x)**2))*(x/np.norm(x))\n        return v\n        ","execution_count":72,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DigitCaps(Model):\n    \n    def __init__(self, num_capsules=10, \n                 num_routes=16*6*6, \n                 in_channels=2, out_channels=16):\n        \n        super().__init__()\n        \n        self.num_capsules = num_capsules\n        self.num_routes = num_routes\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        \n        # maybe?\n        self.W = tf.Variable(np.random.randn(\n            1, self.num_routes,\n            self.num_capsules,\n            self.out_channels,\n            self.in_channels))\n        \n    def call(self, x):\n        \n        b_size = x.shape[0]\n        \n        x = np.stack(\n            [x]*self.num_capsules, axis=2).expand_dims(4)\n        W = np.concatenate([self.W]*b_size, axis=0)\n        u_hat = tf.squeeze(np.dot(W, x))\n        b_ij = tf.Variable(np.zeros(\n            1, self.num_routes, self.num_capsules, 1))\n        c_ij = tf.math.softplus(b_ij) #, dim=1)\n        s_j = (c_ij*u_hat).sum(axis=1) #, keepdim=True)\n        v_j = self.squash(s_j)\n        \n        return v_j.squeeze(1)\n\n    def squash(self, x):\n        v = (np.norm(x)**2/(\n            1+np.norm(x)**2))*(x/np.norm(x))\n        return v","execution_count":73,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Encoder(Model):\n    def __init__(self):\n        \n        super().__init__()\n        \n        self.conv_layer = layers.Conv2D(32, (9, 9), strides=(1, 1))\n        self.primary_capsules = PrimaryCaps()\n        self.digit_capsules = DigitCaps()\n    \n    def call(self, x):\n        x = tf.nn.relu(self.conv_layer(x))\n        x = self.primary_capsules(x)\n        return self.digit_capsules(x)","execution_count":74,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def classifier(classes=10):\n    return Sequential(layers.Flatten(),\n                         layers.Dense(classes))","execution_count":75,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CapsNetForClassification(Model):\n    \n    def __init__(self, num_classes=10): #, input_shape=(28, 28)):\n        \n        super().__init__()\n        \n        #self.input_shape = input_shape        \n        self.encoder = Encoder()\n        self.num_classes = num_classes\n        self.classifier = classifier(10)\n    \n    def call(self, x):\n        x = self.encoder(x)\n        x = self.classifier(x)\n        return x","execution_count":76,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = CapsNetForClassification()\nmodel.build(input_shape=(None, 28, 28, 1))","execution_count":78,"outputs":[{"output_type":"stream","text":"(None, 20, 20, 32)\n","name":"stdout"},{"output_type":"error","ename":"NotImplementedError","evalue":"in user code:\n\n    <ipython-input-54-e8bae7a7bbb8>:12 call  *\n        x = self.primary_capsules(x)\n    <ipython-input-41-a94a1037fce7>:18 call  *\n        caps = np.stack(caps, axis=1)\n    <__array_function__ internals>:6 stack  **\n        \n    /opt/conda/lib/python3.7/site-packages/numpy/core/shape_base.py:420 stack\n        arrays = [asanyarray(arr) for arr in arrays]\n    /opt/conda/lib/python3.7/site-packages/numpy/core/shape_base.py:420 <listcomp>\n        arrays = [asanyarray(arr) for arr in arrays]\n    /opt/conda/lib/python3.7/site-packages/numpy/core/_asarray.py:138 asanyarray\n        return array(a, dtype, copy=False, order=order, subok=True)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:848 __array__\n        \" a NumPy call, which is not supported\".format(self.name))\n\n    NotImplementedError: Cannot convert a symbolic Tensor (encoder_17/primary_caps_17/conv2d_33/BiasAdd:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-78-cc56500154af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCapsNetForClassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    430\u001b[0m                            'method accepts an `inputs` argument.')\n\u001b[1;32m    431\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m           raise ValueError('You cannot build your model by calling `build` '\n","\u001b[0;32m<ipython-input-76-f24be25fbecb>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: in user code:\n\n    <ipython-input-54-e8bae7a7bbb8>:12 call  *\n        x = self.primary_capsules(x)\n    <ipython-input-41-a94a1037fce7>:18 call  *\n        caps = np.stack(caps, axis=1)\n    <__array_function__ internals>:6 stack  **\n        \n    /opt/conda/lib/python3.7/site-packages/numpy/core/shape_base.py:420 stack\n        arrays = [asanyarray(arr) for arr in arrays]\n    /opt/conda/lib/python3.7/site-packages/numpy/core/shape_base.py:420 <listcomp>\n        arrays = [asanyarray(arr) for arr in arrays]\n    /opt/conda/lib/python3.7/site-packages/numpy/core/_asarray.py:138 asanyarray\n        return array(a, dtype, copy=False, order=order, subok=True)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:848 __array__\n        \" a NumPy call, which is not supported\".format(self.name))\n\n    NotImplementedError: Cannot convert a symbolic Tensor (encoder_17/primary_caps_17/conv2d_33/BiasAdd:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\n"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=\"adam\",\n    loss='categorical_crossentropy',\n    metrics=['accuracy', metrics.Precision(), metrics.Recall()])\nmodel.fit(x_train, y_train)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}